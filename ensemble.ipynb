{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n!pip install natsort   \nfrom natsort import natsorted\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport cv2\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport torch\nimport torchaudio\nimport torchaudio.functional as F\nimport torchaudio.transforms as T\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport numpy as np\nimport torchvision\nfrom torchvision import datasets, models, transforms\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom os.path import dirname, join as pjoin\nfrom scipy.io import wavfile\nimport scipy.io\nimport matplotlib.pyplot as plt\nfrom scipy import signal\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nimport xgboost as xgb\nfrom sklearn.datasets import make_multilabel_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn import preprocessing\nfrom PIL import Image\nimport os\nimport pandas as pd\nfrom torch.utils.data.dataset import Dataset\nfrom torchvision import transforms\nimport numpy as np\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n    #for filename in filenames:\n        #print(os.path.join(dirname, filename))\ntrain_files='../input/spoken-digit-pair-recognition/train/train_new/'\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-06T03:40:18.883013Z","iopub.execute_input":"2021-12-06T03:40:18.883278Z","iopub.status.idle":"2021-12-06T03:40:30.797799Z","shell.execute_reply.started":"2021-12-06T03:40:18.883242Z","shell.execute_reply":"2021-12-06T03:40:30.797011Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting natsort\n  Downloading natsort-8.0.0-py3-none-any.whl (37 kB)\nInstalling collected packages: natsort\nSuccessfully installed natsort-8.0.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"Xlist=[]\nfor single_image_name in natsorted(os.listdir(train_files)):\n    samplerate, data = wavfile.read(train_files + single_image_name)\n    f, t, img_as_img = signal.spectrogram(data, samplerate)\n    Xlist.append(img_as_img)\nX=np.array(Xlist)\nX=X.reshape(90000,3354)    ","metadata":{"execution":{"iopub.status.busy":"2021-12-02T21:08:06.598859Z","iopub.execute_input":"2021-12-02T21:08:06.59922Z","iopub.status.idle":"2021-12-02T21:14:07.770301Z","shell.execute_reply.started":"2021-12-02T21:08:06.599177Z","shell.execute_reply":"2021-12-02T21:14:07.769024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=pd.read_csv(\"../input/spoken-digit-pair-recognition/train.csv\")\ny1=data['Label'].values","metadata":{"execution":{"iopub.status.busy":"2021-12-06T03:40:33.347436Z","iopub.execute_input":"2021-12-06T03:40:33.347715Z","iopub.status.idle":"2021-12-06T03:40:33.382681Z","shell.execute_reply.started":"2021-12-06T03:40:33.347683Z","shell.execute_reply":"2021-12-06T03:40:33.382096Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"dictionary={21:[1,1,0,0], 31:[1,0,1,0], 32:[0,1,1,0], 41:[1,0,0,1], 42:[0,1,0,1], 43:[0,0,1,1]}\nY = [dictionary[letter] for letter in y1]\n#print(Y)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T03:40:36.473140Z","iopub.execute_input":"2021-12-06T03:40:36.473644Z","iopub.status.idle":"2021-12-06T03:40:36.542089Z","shell.execute_reply.started":"2021-12-06T03:40:36.473575Z","shell.execute_reply":"2021-12-06T03:40:36.541437Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"dictionary3={21:0, 31:1, 32:1, 41:0, 42:0, 43:1}\nY3 = [dictionary3[letter] for letter in y1]","metadata":{"execution":{"iopub.status.busy":"2021-12-06T03:40:40.141875Z","iopub.execute_input":"2021-12-06T03:40:40.142393Z","iopub.status.idle":"2021-12-06T03:40:40.213312Z","shell.execute_reply.started":"2021-12-06T03:40:40.142358Z","shell.execute_reply":"2021-12-06T03:40:40.212604Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"dictionary4={21:0, 31:0, 32:0, 41:1, 42:1, 43:1}\nY4 = [dictionary4[letter] for letter in y1]","metadata":{"execution":{"iopub.status.busy":"2021-12-06T03:40:42.748995Z","iopub.execute_input":"2021-12-06T03:40:42.749409Z","iopub.status.idle":"2021-12-06T03:40:42.817737Z","shell.execute_reply.started":"2021-12-06T03:40:42.749379Z","shell.execute_reply":"2021-12-06T03:40:42.817087Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"dictionary1={21:1, 31:1, 32:0, 41:1, 42:0, 43:0}\nY1 = [dictionary1[letter] for letter in y1]","metadata":{"execution":{"iopub.status.busy":"2021-12-03T18:55:16.468125Z","iopub.execute_input":"2021-12-03T18:55:16.468526Z","iopub.status.idle":"2021-12-03T18:55:16.508999Z","shell.execute_reply.started":"2021-12-03T18:55:16.468487Z","shell.execute_reply":"2021-12-03T18:55:16.50836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dictionary2={21:1, 31:0, 32:1, 41:0, 42:1, 43:0}\nY2 = [dictionary2[letter] for letter in y1]","metadata":{"execution":{"iopub.status.busy":"2021-12-03T18:55:17.790993Z","iopub.execute_input":"2021-12-03T18:55:17.791275Z","iopub.status.idle":"2021-12-03T18:55:17.828839Z","shell.execute_reply.started":"2021-12-03T18:55:17.791245Z","shell.execute_reply":"2021-12-03T18:55:17.827963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.save('X.npy', X)\nnp.save('Y.npy', y1)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-02T21:16:19.093966Z","iopub.execute_input":"2021-12-02T21:16:19.094466Z","iopub.status.idle":"2021-12-02T21:16:20.497807Z","shell.execute_reply.started":"2021-12-02T21:16:19.094423Z","shell.execute_reply":"2021-12-02T21:16:20.496965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=np.load('../input/numpycomp/X.npy')","metadata":{"execution":{"iopub.status.busy":"2021-12-06T03:40:52.358748Z","iopub.execute_input":"2021-12-06T03:40:52.359444Z","iopub.status.idle":"2021-12-06T03:41:09.880812Z","shell.execute_reply.started":"2021-12-06T03:40:52.359400Z","shell.execute_reply":"2021-12-06T03:41:09.879849Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, Y1, test_size=0.00001)\n\n# create XGBoost instance with default hyper-parameters\nxgb_estimator1 = xgb.XGBClassifier(objective='binary:logistic').fit(X_train, y_train)\n#y_predtr3=xgb_estimator3.predict(X_test)\n# evaluate on test data\nprint('Done')","metadata":{"execution":{"iopub.status.busy":"2021-12-03T19:04:22.751808Z","iopub.execute_input":"2021-12-03T19:04:22.752353Z","iopub.status.idle":"2021-12-03T19:25:40.900014Z","shell.execute_reply.started":"2021-12-03T19:04:22.752306Z","shell.execute_reply":"2021-12-03T19:25:40.898267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, Y2, test_size=0.00001)\n\n# create XGBoost instance with default hyper-parameters\nxgb_estimator2 = xgb.XGBClassifier(objective='binary:logistic').fit(X_train, y_train)\n#y_predtr3=xgb_estimator3.predict(X_test)\n# evaluate on test data\nprint('Done')","metadata":{"execution":{"iopub.status.busy":"2021-12-03T19:25:40.905243Z","iopub.execute_input":"2021-12-03T19:25:40.905687Z","iopub.status.idle":"2021-12-03T19:47:07.353599Z","shell.execute_reply.started":"2021-12-03T19:25:40.905647Z","shell.execute_reply":"2021-12-03T19:47:07.352656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, Y3, test_size=0.00001)\n\n# create XGBoost instance with default hyper-parameters\nxgb_estimator3 = xgb.XGBClassifier(verbosity=2, objective='binary:hinge',max_depth=7,alpha=1,reg_lambda=2,num_parallel_tree=2).fit(X_train, y_train)\nprint('done')\n#from sklearn.svm import LinearSVC\n#Create a svm Classifier\n#svm = LinearSVC().fit(X_train, y_train)\n# Model Accuracy: how often is the classifier correct?\nprint('done')","metadata":{"execution":{"iopub.status.busy":"2021-12-04T04:15:59.375209Z","iopub.execute_input":"2021-12-04T04:15:59.375503Z","iopub.status.idle":"2021-12-04T04:46:26.230816Z","shell.execute_reply.started":"2021-12-04T04:15:59.375472Z","shell.execute_reply":"2021-12-04T04:46:26.22995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, Y4, test_size=0.00001)\nxgb_estimator4 = xgb.XGBClassifier(verbosity=2, objective='binary:hinge',max_depth=7,alpha=1,reg_lambda=2,num_parallel_tree=2).fit(X_train, y_train)\n#xgb_estimator4 = xgb.XGBClassifier(objective='binary:logistic').fit(X_train, y_train)\nprint('Done')","metadata":{"execution":{"iopub.status.busy":"2021-12-04T04:46:26.232289Z","iopub.execute_input":"2021-12-04T04:46:26.233498Z","iopub.status.idle":"2021-12-04T05:13:13.379168Z","shell.execute_reply.started":"2021-12-04T04:46:26.233469Z","shell.execute_reply":"2021-12-04T05:13:13.378349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.datasets import make_multilabel_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.metrics import accuracy_score\n\n# create sample dataset\n#X, y = make_multilabel_classification(n_samples=3000, n_features=45, n_classes=20, n_labels=1,\n                                      #allow_unlabeled=False, random_state=42)\n\n# split dataset into training and test set\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.00001)\n\n# create XGBoost instance with default hyper-parameters\nxgb_estimatortotal = xgb.XGBClassifier(objective='binary:hinge',verbosity=2,num_parallel_tree=2)\n\n# create MultiOutputClassifier instance with XGBoost model inside\nmultilabel_model = MultiOutputClassifier(xgb_estimatortotal)\n\n# fit the model\nmultilabel_model.fit(X_train, y_train)\nprint('Done')","metadata":{"execution":{"iopub.status.busy":"2021-12-04T05:13:13.380548Z","iopub.execute_input":"2021-12-04T05:13:13.380861Z","iopub.status.idle":"2021-12-04T07:25:50.907813Z","shell.execute_reply.started":"2021-12-04T05:13:13.380816Z","shell.execute_reply":"2021-12-04T07:25:50.906664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nX_train, X_test, y_train, y_test = train_test_split(X, Y4, test_size=0.00001)\n\nxgb_estimator4 = RandomForestClassifier().fit(X_train, y_train)\n\nprint('Done')\nX_train, X_test, y_train, y_test = train_test_split(X, Y3, test_size=0.00001)\n\n# create XGBoost instance with default hyper-parameters\nxgb_estimator3 = RandomForestClassifier().fit(X_train, y_train)\nprint('Done')","metadata":{"execution":{"iopub.status.busy":"2021-12-03T22:27:41.466739Z","iopub.execute_input":"2021-12-03T22:27:41.467058Z","iopub.status.idle":"2021-12-03T22:47:22.900471Z","shell.execute_reply.started":"2021-12-03T22:27:41.467025Z","shell.execute_reply":"2021-12-03T22:47:22.899377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Can train on full dataset for marginally better results","metadata":{}},{"cell_type":"code","source":"#2 NNs\nimport pandas as pd\nfrom torch.utils.data.dataset import Dataset\nfrom torchvision import transforms\nimport numpy as np\nclass SimpleDataset3(Dataset):\n    def __init__(self, data_path, csv_name, transform = None ):\n        \"\"\"\n        Args:\n            data_path (string): path to the folder where images and csv files are located\n            csv_name (string): name of the csv lablel file\n            transform: pytorch transforms for transforms and tensor conversion\n        \"\"\"\n        # Set path\n        self.data_path = data_path\n        # Transforms\n        self.transform = transform\n        \n        # Read the csv file\n        self.data_info = pd.read_csv(csv_name)\n        # First column contains the image paths\n        self.image_arr = np.asarray(self.data_info.iloc[:, 0])\n        # Second column is the labels\n        self.label_arr = np.asarray(self.data_info.iloc[:, 1])\n        #print(self.label_arr)\n        # Calculate len\n        self.data_len = len(self.data_info)\n        \n\n    def __getitem__(self, index):\n        # Get image name from the pandas df\n        single_image_name = \"/train_\"+np.array2string(self.image_arr[index]) +\".wav\"\n        # Open image\n        samplerate, data = wavfile.read(self.data_path + single_image_name)\n        f, t, img_as_img = signal.spectrogram(data, samplerate)\n        #Sxx.flatten().shape\n        #x = torch.flatten(self.data_path[index])\n        #print(x.shape)\n        #img_as_img = x.expand(3, 128,31)\n        #img_as_img=x\n        if self.transform is not None:\n              img_as_img = self.transform(img_as_img)\n\n        # Get label(class) of the image based on the cropped pandas column\n        #train_tensor = torch.tensor(data['Label'].values)\n        single_image_label =  self.label_arr[index]\n        dictionary={21:0, 31:1, 32:1, 41:0, 42:0, 43:1}\n        \n        #You could create a dict inside your Dataset and map the target strings to the class indices in the __getitem__ method.\n        #convert to tensor to be consistent with MNIST dataset\n        single_image_label = torch.from_numpy(np.asarray(dictionary[single_image_label]))\n        #print(single_image_label)\n        return (img_as_img, single_image_label)\n\n    def __len__(self):\n        return self.data_len\n\nmydata = SimpleDataset3( '../input/spoken-digit-pair-recognition/train/train_new', \"../input/spoken-digit-pair-recognition/train.csv\",\n                       transform=transforms.Compose([\n                        transforms.ToTensor()\n                        #transforms.Resize(size=(224,224))\n                        \n\n]))\n#len(mydata)\n#mydataT = SimpleDataset( \"idata/\", \"labels.csv\", transform=transforms.ToTensor())\ntest_loader = torch.utils.data.DataLoader(dataset=mydata,\n                                          batch_size=16, \n                                          shuffle=True)\n\nnum_classes=2\nclass ConvNet(nn.Module):\n    def __init__(self, num_classes=2):\n        super(ConvNet, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(1,16, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(16),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        self.fc = nn.Linear(6144, num_classes)\n        \n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = out.reshape(out.size(0), -1)\n        out = self.fc(out)\n        return out\n    \n\nmodel =models.resnet18(pretrained=False)\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(num_ftrs, 2)\ncriterion = nn.CrossEntropyLoss()\nmodel.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n# Observe that all parameters are being optimized\noptimizer = optim.SGD(model.parameters(), lr=0.005, weight_decay=0.01)\n\nnum_epochs=5\ntotal_step = len(test_loader)\nfor epoch in range(num_epochs):\n    for i, (images,labels) in enumerate(test_loader):\n\n        # Forward pass\n        outputs = model(images)\n        #print(f\"{torch.argmax(outputs,dim=1)} and {labels}\")\n        loss = criterion(outputs, labels)\n        \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 100 == 0:\n            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n\ntorch.save(model.state_dict(), 'modelconv3.ckpt')\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-06T03:40:15.710315Z","iopub.execute_input":"2021-12-06T03:40:15.710636Z","iopub.status.idle":"2021-12-06T03:40:17.342129Z","shell.execute_reply.started":"2021-12-06T03:40:15.710548Z","shell.execute_reply":"2021-12-06T03:40:17.340927Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_34/731989813.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;31m#len(mydata)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m#mydataT = SimpleDataset( \"idata/\", \"labels.csv\", transform=transforms.ToTensor())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m test_loader = torch.utils.data.DataLoader(dataset=mydata,\n\u001b[0m\u001b[1;32m     68\u001b[0m                                           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                                           shuffle=True)\n","\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"],"ename":"NameError","evalue":"name 'torch' is not defined","output_type":"error"}]},{"cell_type":"code","source":"class SimpleTestDataset(Dataset):\n    def __init__(self, data_path, transform = None ):\n        \"\"\"\n        Args:\n            data_path (string): path to the folder where images and csv files are located\n            csv_name (string): name of the csv lablel file\n            transform: pytorch transforms for transforms and tensor conversion\n        \"\"\"\n        # Set path\n        self.data_path = data_path\n        # Transforms\n        self.transform = transform\n        \n        # Read the csv file\n        #self.data_info = pd.read_csv(csv_name)\n        # First column contains the image paths\n        #self.image_arr = np.asarray(self.data_info.iloc[:, 0])\n        # Second column is the labels\n        #self.label_arr = np.asarray(self.data_info.iloc[:, 1])\n        #print(self.label_arr)\n        # Calculate len\n        self.folder=natsorted(os.listdir(data_path))\n        self.data_len = len(self.folder)\n        \n\n    def __getitem__(self, index):\n        # Get image name from the pandas df\n        #single_image_name = \"/test_\"+str(index)+\".wav\"\n        single_image_name =self.folder[index]\n        #print(single_image_name)\n        # Open image\n        samplerate, data = wavfile.read(self.data_path + single_image_name)\n        f, t, img_as_img = signal.spectrogram(data, samplerate)\n        #print(img_as_img)\n        #Sxx.flatten().shape\n        #x = torch.flatten(self.data_path[index])\n        #print(x.shape)\n        #img_as_img = x.expand(3, 128,31)\n        #img_as_img=x\n        if self.transform is not None:\n              img_as_img = self.transform(img_as_img)\n\n        # Get label(class) of the image based on the cropped pandas column\n        #train_tensor = torch.tensor(data['Label'].values)\n        #single_image_label =  self.label_arr[index]\n        #dictionary={21:0, 31:1, 32:2, 41:3, 42:4, 43:5}\n        \n        #You could create a dict inside your Dataset and map the target strings to the class indices in the __getitem__ method.\n        #convert to tensor to be consistent with MNIST dataset\n        #single_image_label = torch.from_numpy(np.asarray(dictionary[single_image_label]))\n        #print(single_image_label)\n        return (img_as_img)\n\n    def __len__(self):\n        return self.data_len\n\nmydataTest = SimpleTestDataset( '../input/spoken-digit-pair-recognition/test/test_new/',\n                       transform=transforms.Compose([\n                        transforms.ToTensor()\n                        #transforms.Resize(size=(224,224))\n                        \n\n]))\n#len(mydata)\n#mydataT = SimpleDataset( \"idata/\", \"labels.csv\", transform=transforms.ToTensor())\ntestest = torch.utils.data.DataLoader(dataset=mydataTest,\n                                          batch_size=1, \n                                          shuffle=False)\nmodel =models.resnet18()\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(num_ftrs, 2)\ncriterion = nn.CrossEntropyLoss()\nmodel.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\nmodel.load_state_dict(torch.load('../input/resnets/resnet3.ckpt'))\nmodel.eval()\npredictions3=[]\naid=[]\n#decode_dictionary={0:21, 1:31, 2:32, 3:41, 4:42, 5:43}\n\nmodel.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\nwith torch.no_grad():\n    for i,images in enumerate(testest):\n        outputs = model(images)\n        #print(i)\n        #print(images)\n        #print(outputs)\n        #outputs.to\n        #pr=torch.softmax(outputs,dim=1)\n        #print(pr)\n        #if pr[0,1].item()>0.89:\n         #   preds=torch.tensor(1)\n            #print(preds)\n        #else:\n        preds = torch.argmax(outputs)\n            #print(preds)\n        aid.append(i)\n        predictions3.append(preds.item())","metadata":{"execution":{"iopub.status.busy":"2021-12-06T03:42:49.720967Z","iopub.execute_input":"2021-12-06T03:42:49.721926Z","iopub.status.idle":"2021-12-06T03:50:20.842353Z","shell.execute_reply.started":"2021-12-06T03:42:49.721843Z","shell.execute_reply":"2021-12-06T03:50:20.841590Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#2 NNs\nimport pandas as pd\nfrom torch.utils.data.dataset import Dataset\nfrom torchvision import transforms\nimport numpy as np\nclass SimpleDataset4(Dataset):\n    def __init__(self, data_path, csv_name, transform = None ):\n        \"\"\"\n        Args:\n            data_path (string): path to the folder where images and csv files are located\n            csv_name (string): name of the csv lablel file\n            transform: pytorch transforms for transforms and tensor conversion\n        \"\"\"\n        # Set path\n        self.data_path = data_path\n        # Transforms\n        self.transform = transform\n        \n        # Read the csv file\n        self.data_info = pd.read_csv(csv_name)\n        # First column contains the image paths\n        self.image_arr = np.asarray(self.data_info.iloc[:, 0])\n        # Second column is the labels\n        self.label_arr = np.asarray(self.data_info.iloc[:, 1])\n        #print(self.label_arr)\n        # Calculate len\n        self.data_len = len(self.data_info)\n        \n\n    def __getitem__(self, index):\n        # Get image name from the pandas df\n        single_image_name = \"/train_\"+np.array2string(self.image_arr[index]) +\".wav\"\n        # Open image\n        samplerate, data = wavfile.read(self.data_path + single_image_name)\n        f, t, img_as_img = signal.spectrogram(data, samplerate)\n        #Sxx.flatten().shape\n        #x = torch.flatten(self.data_path[index])\n        #print(x.shape)\n        #img_as_img = x.expand(3, 128,31)\n        #img_as_img=x\n        if self.transform is not None:\n              img_as_img = self.transform(img_as_img)\n\n        # Get label(class) of the image based on the cropped pandas column\n        #train_tensor = torch.tensor(data['Label'].values)\n        single_image_label =  self.label_arr[index]\n        dictionary={21:0, 31:0, 32:0, 41:1, 42:1, 43:1}\n        \n        #You could create a dict inside your Dataset and map the target strings to the class indices in the __getitem__ method.\n        #convert to tensor to be consistent with MNIST dataset\n        single_image_label = torch.from_numpy(np.asarray(dictionary[single_image_label]))\n        #print(single_image_label)\n        return (img_as_img, single_image_label)\n\n    def __len__(self):\n        return self.data_len\n\nmydata = SimpleDataset4( '../input/spoken-digit-pair-recognition/train/train_new', \"../input/spoken-digit-pair-recognition/train.csv\",transform=transforms.ToTensor())\n                       #transform=transforms.Compose([\n   # transforms.Normalize(mean=[mean],\n    #                     std=[0.229])\n# ]))\n#len(mydata)\n#mydataT = SimpleDataset( \"idata/\", \"labels.csv\", transform=transforms.ToTensor())\ntest_loader4 = torch.utils.data.DataLoader(dataset=mydata,\n                                          batch_size=16, \n                                          shuffle=True)\n\nnum_classes=2\nmodel2 =models.resnet18(pretrained=False)\nnum_ftrs = model2.fc.in_features\nmodel2.fc = nn.Linear(num_ftrs, 2)\ncriterion = nn.CrossEntropyLoss()\nmodel2.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\ncriterion = nn.CrossEntropyLoss()\n\n# Observe that all parameters are being optimized\noptimizer = optim.SGD(model2.parameters(), lr=0.005, weight_decay=0.01)\n\nnum_epochs=3\ntotal_step = len(test_loader4)\nfor epoch in range(num_epochs):\n    for i, (images,labels) in enumerate(test_loader4):\n\n        # Forward pass\n        outputs = model2(images)\n        #print(f\"{torch.argmax(outputs,dim=1)} and {labels}\")\n        loss = criterion(outputs, labels)\n        \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 100 == 0:\n            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n\ntorch.save(model2.state_dict(), 'modelconv4.ckpt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model2 =models.resnet18()\nnum_ftrs = model2.fc.in_features\nmodel2.fc = nn.Linear(num_ftrs, 2)\n#criterion = nn.CrossEntropyLoss()\nmodel2.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\nmodel2.load_state_dict(torch.load('../input/resnets/resnet4.ckpt'))\nmodel2.eval()\npredictions4=[]\naid=[]\n#decode_dictionary={0:21, 1:31, 2:32, 3:41, 4:42, 5:43}\n\nmodel2.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\nwith torch.no_grad():\n    for i,images in enumerate(testest):\n        outputs = model2(images)\n        #print(i)\n        #print(images)\n        #print(outputs)\n        #outputs.to\n        #pr=torch.softmax(outputs,dim=1)\n        #print(pr)\n        #if pr[0,1].item()>0.89:\n         #   preds=torch.tensor(1)\n            #print(preds)\n        #else:\n        preds = torch.argmax(outputs)\n            #print(preds)\n        aid.append(i)\n        predictions4.append(preds.item())","metadata":{"execution":{"iopub.status.busy":"2021-12-06T03:50:20.844416Z","iopub.execute_input":"2021-12-06T03:50:20.845320Z","iopub.status.idle":"2021-12-06T03:55:34.616469Z","shell.execute_reply.started":"2021-12-06T03:50:20.845274Z","shell.execute_reply":"2021-12-06T03:55:34.615909Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class SimpleDatasetFull(Dataset):\n    def __init__(self, data_path, csv_name, transform = None ):\n        \"\"\"\n        Args:\n            data_path (string): path to the folder where images and csv files are located\n            csv_name (string): name of the csv lablel file\n            transform: pytorch transforms for transforms and tensor conversion\n        \"\"\"\n        # Set path\n        self.data_path = data_path\n        # Transforms\n        self.transform = transform\n        \n        # Read the csv file\n        self.data_info = pd.read_csv(csv_name)\n        # First column contains the image paths\n        self.image_arr = np.asarray(self.data_info.iloc[:, 0])\n        # Second column is the labels\n        self.label_arr = np.asarray(self.data_info.iloc[:, 1])\n        #print(self.label_arr)\n        # Calculate len\n        self.data_len = len(self.data_info)\n        \n\n    def __getitem__(self, index):\n        # Get image name from the pandas df\n        single_image_name = \"/train_\"+np.array2string(self.image_arr[index]) +\".wav\"\n        # Open image\n        samplerate, data = wavfile.read(self.data_path + single_image_name)\n        f, t, img_as_img = signal.spectrogram(data, samplerate)\n        #Sxx.flatten().shape\n        #x = torch.flatten(self.data_path[index])\n        #print(x.shape)\n        #img_as_img = x.expand(3, 128,31)\n        #img_as_img=x\n        if self.transform is not None:\n              img_as_img = self.transform(img_as_img)\n\n        # Get label(class) of the image based on the cropped pandas column\n        #train_tensor = torch.tensor(data['Label'].values)\n        single_image_label =  self.label_arr[index]\n        dictionary={21:[1,1,0,0], 31:[1,0,1,0], 32:[0,1,1,0], 41:[1,0,0,1], 42:[0,1,0,1], 43:[0,0,1,1]}\n        \n        #You could create a dict inside your Dataset and map the target strings to the class indices in the __getitem__ method.\n        #convert to tensor to be consistent with MNIST dataset\n        single_image_label = torch.from_numpy(np.asarray(dictionary[single_image_label]))\n        #print(single_image_label)\n        return (img_as_img, single_image_label)\n\n    def __len__(self):\n        return self.data_len\n\nmydata = SimpleDatasetFull( '../input/spoken-digit-pair-recognition/train/train_new', \"../input/spoken-digit-pair-recognition/train.csv\",\n    transform=transforms.Compose([\n    transforms.ToTensor()\n    #transforms.Normalize(mean=[mean],\n                        #std=[0.229])\n ]))\n#len(mydata)\n#mydataT = SimpleDataset( \"idata/\", \"labels.csv\", transform=transforms.ToTensor())\ntest_loader = torch.utils.data.DataLoader(dataset=mydata,\n                                          batch_size=16, \n                                          shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T03:55:34.617722Z","iopub.execute_input":"2021-12-06T03:55:34.618076Z","iopub.status.idle":"2021-12-06T03:55:34.652184Z","shell.execute_reply.started":"2021-12-06T03:55:34.618037Z","shell.execute_reply":"2021-12-06T03:55:34.651611Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"modelfull =models.resnet18(pretrained=False)\nnum_ftrs = modelfull.fc.in_features\nmodelfull.fc = nn.Linear(num_ftrs, 4)\n\nmodelfull.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\ncriterion = nn.BCEWithLogitsLoss()\n\n# Observe that all parameters are being optimized\noptimizer = optim.SGD(modelfull.parameters(), lr=0.01, weight_decay=0.01)\n\nnum_epochs=3\ntotal_step = len(test_loader)\nfor epoch in range(num_epochs):\n    for i, (images,labels) in enumerate(test_loader):\n\n        # Forward pass\n        outputs = modelfull(images)\n        #print(f\"{torch.argmax(outputs,dim=1)} and {labels}\")\n        loss = criterion(outputs, labels.float())\n        \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 100 == 0:\n            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmodelfull =models.resnet18(pretrained=False)\n\nnum_ftrs = modelfull.fc.in_features\nmodelfull.fc = nn.Linear(num_ftrs, 4)\ncriterion = nn.CrossEntropyLoss()\nmodelfull.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\nmodelfull.load_state_dict(torch.load('../input/resnets/modelfull.ckpt'))\n#torch.save(modelfull.state_dict(), 'modelfull.ckpt')\nmodelfull.eval()\npredictions=[]\naid=[]\n#decode_dictionary={0:21, 1:31, 2:32, 3:41, 4:42, 5:43}\n\nmodelfull.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\nwith torch.no_grad():\n    for i,images in enumerate(testest):\n        outputs = modelfull(images)\n        #pred=0\n        #print(i)\n        #print(images)\n        #print(outputs)\n        #outputs.to\n        output_prob = torch.sigmoid(outputs)\n        pred=output_prob.tolist()# calculate probabilities\n        #print(output_prob)\n        #preds=(torch.topk(output_prob, 2).indices).tolist()\n        #predss = np.asarray(output_prob > 0.5)\n        #preds=predss\n        #print(preds)\n        #print(preds[0][0])\n        #if(preds[0].count(0)>0 and preds[0].count(1)>0):\n         #   pred=21\n        #elif(preds[0].count(0)>0 and preds[0].count(2)>0):\n         #   pred=31\n        #elif(preds[0].count(0)>0 and preds[0].count(3)>0):\n         #   pred=41\n        #elif(preds[0].count(1)>0 and preds[0].count(2)>0):\n         #   pred=32\n        #elif(preds[0].count(1)>0 and preds[0].count(3)>0):\n         #   pred=42\n        #elif(preds[0].count(2)>0 and preds[0].count(3)>0):\n         #   pred=43\n        #print(pred)\n        #print(pred)\n        #preds=preds.tolist()\n        #pr=torch.softmax(outputs,dim=1)\n        #print(pr)\n        #if pr[0,0].item()<0.11 and pr[0,1].item()<0.89 and pr[0,2].item()<0.89 and pr[0,3].item()<0.89 and pr[0,4].item()<0.89:\n         #   preds=torch.tensor(5)\n            #print(preds)\n        #else:\n          #  preds = torch.argmax(outputs)\n            #print(preds)\n        aid.append(i)\n        predictions.append(pred)\n        ","metadata":{"execution":{"iopub.status.busy":"2021-12-06T04:00:50.185803Z","iopub.execute_input":"2021-12-06T04:00:50.186412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nxgb_estimator = RandomForestClassifier(random_state=0)\n\n# create MultiOutputClassifier instance with XGBoost model inside\nmultilabel_model = MultiOutputClassifier(xgb_estimator)\n\n# fit the model\nmultilabel_model.fit(X_train, y_train)\n\n# evaluate on test data\nprint('Accuracy on test data: {:.1f}%'.format(accuracy_score(y_test, multilabel_model.predict(X_test))*100))","metadata":{"execution":{"iopub.status.busy":"2021-12-02T22:50:29.720277Z","iopub.execute_input":"2021-12-02T22:50:29.721536Z","iopub.status.idle":"2021-12-02T23:24:03.470858Z","shell.execute_reply.started":"2021-12-02T22:50:29.721485Z","shell.execute_reply":"2021-12-02T23:24:03.469862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_files=\"../input/spoken-digit-pair-recognition/test/test_new/\"\nXTlist=[]\nfor single_image_name in natsorted(os.listdir(test_files)):\n    samplerate, data = wavfile.read(test_files + single_image_name)\n    f, t, img_as_img = signal.spectrogram(data, samplerate)\n    XTlist.append(img_as_img)","metadata":{"execution":{"iopub.status.busy":"2021-12-04T07:25:50.909783Z","iopub.execute_input":"2021-12-04T07:25:50.909993Z","iopub.status.idle":"2021-12-04T07:28:55.958411Z","shell.execute_reply.started":"2021-12-04T07:25:50.909966Z","shell.execute_reply":"2021-12-04T07:28:55.957662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#X_t=np.array(XTlist)\nX_t=np.load('../input/testnpy/XT.npy')\n#X_t=X_t.reshape(len(os.listdir(test_files)),3354)    \ny_pred=multilabel_model.predict(X_t)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#y_pred=multilabel_model.predict(X_t)\n#y_pred1=xgb_estimator1.predict(X_t)\n#y_pred2=xgb_estimator2.predict(X_t)\ny_pred3=xgb_estimator3.predict(X_t)\ny_pred4=xgb_estimator4.predict(X_t)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from joblib import dump, load\n\ndump(multilabel_model, 'MainModel.joblib') \ndump(xgb_estimator3, '3.joblib') \ndump(xgb_estimator4, '4.joblib') \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from joblib import dump, load\nmultilabel_model = load('../input/xgb-models/MainModel.joblib') \nxgb_estimator3 = load('../input/xgb-models/3.joblib') \nxgb_estimator4 = load('../input/xgb-models/4.joblib') ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.save('XT.npy', X_t)","metadata":{"execution":{"iopub.status.busy":"2021-12-04T07:30:16.319335Z","iopub.execute_input":"2021-12-04T07:30:16.319633Z","iopub.status.idle":"2021-12-04T07:30:16.624761Z","shell.execute_reply.started":"2021-12-04T07:30:16.3196Z","shell.execute_reply":"2021-12-04T07:30:16.623903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred[100]","metadata":{"execution":{"iopub.status.busy":"2021-12-02T22:43:32.417213Z","iopub.execute_input":"2021-12-02T22:43:32.417587Z","iopub.status.idle":"2021-12-02T22:43:32.423953Z","shell.execute_reply.started":"2021-12-02T22:43:32.417558Z","shell.execute_reply":"2021-12-02T22:43:32.423224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1)\nfrom sklearn.neighbors import KNeighborsClassifier\nneigh = KNeighborsClassifier(n_neighbors=3)\nmultilabel_model = MultiOutputClassifier(neigh)\n\n# fit the model\nmultilabel_model.fit(X_train, y_train)\n\n# evaluate on test data\nprint('Accuracy on test data: {:.1f}%'.format(accuracy_score(y_test, multilabel_model.predict(X_test))*100))","metadata":{"execution":{"iopub.status.busy":"2021-12-03T03:01:54.484661Z","iopub.execute_input":"2021-12-03T03:01:54.485009Z","iopub.status.idle":"2021-12-03T05:45:59.247992Z","shell.execute_reply.started":"2021-12-03T03:01:54.484972Z","shell.execute_reply":"2021-12-03T05:45:59.246875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred=multilabel_model.predict(X_t)","metadata":{"execution":{"iopub.status.busy":"2021-12-03T06:25:33.209667Z","iopub.execute_input":"2021-12-03T06:25:33.210373Z","iopub.status.idle":"2021-12-03T06:25:33.303086Z","shell.execute_reply.started":"2021-12-03T06:25:33.210236Z","shell.execute_reply":"2021-12-03T06:25:33.301935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_pred=[]\naid=[]\nn=0\nfor preds in y_pred:\n    if(preds[0]==1 and preds[1]==1 and preds[2]==0 and preds[3]==0):\n        pred=21\n    elif(preds[0]==1 and preds[1]==0 and preds[2]==1 and preds[3]==0):\n        pred=31\n    elif(preds[0]==0 and preds[1]==1 and preds[2]==1 and preds[3]==0):\n        pred=32\n    elif(preds[0]==1 and preds[1]==0 and preds[2]==0 and preds[3]==1):\n        pred=41\n    elif(preds[0]==0 and preds[1]==1 and preds[2]==0 and preds[3]==1):\n        pred=42\n    elif(preds[0]==0 and preds[1]==0 and preds[2]==1 and preds[3]==1):\n        pred=43\n    aid.append(n)\n    final_pred.append(pred)\n    n+=1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count=0\nfor i in range(len(final_pred)):\n    if (final_pred[i]!=21 and y_pred1[i]==1 and y_pred2[i]==1 and predictions3[i]==0 and predictions4[i]==0):\n        final_pred[i]=21\n        count+=1\n    elif (final_pred[i]!=31 and y_pred1[i]==1 and y_pred2[i]==0 and predictions3[i]==1 and predictions4[i]==0):\n        final_pred[i]=31\n        count+=1\n    elif (final_pred[i]!=32 and y_pred1[i]==0 and y_pred2[i]==1 and predictions3[i]==1 and predictions4[i]==0):\n        final_pred[i]=32\n        count+=1\n    elif (final_pred[i]!=41 and y_pred1[i]==1 and y_pred2[i]==0 and predictions3[i]==0 and predictions4[i]==1):\n        final_pred[i]=41\n        count+=1\n    elif (final_pred[i]!=42 and y_pred1[i]==0 and y_pred2[i]==1 and predictions3[i]==0 and predictions4[i]==1):\n        final_pred[i]=42\n        count+=1\n    elif (final_pred[i]!=43 and predictions3[i]==1 and predictions4[i]==1):\n        final_pred[i]=43\n        count+=1\n    \ncount","metadata":{"execution":{"iopub.status.busy":"2021-12-04T00:11:07.35478Z","iopub.execute_input":"2021-12-04T00:11:07.355114Z","iopub.status.idle":"2021-12-04T00:11:07.441177Z","shell.execute_reply.started":"2021-12-04T00:11:07.35508Z","shell.execute_reply":"2021-12-04T00:11:07.440148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count=0\nfor i in range(len(final_pred)):\n    if (final_pred[i]!=21 and y_pred1[i]==1 and y_pred2[i]==1):\n        final_pred[i]=21\n        count+=1\n    elif (final_pred[i]!=43 and predictions3[i]==1 and predictions4[i]==1):\n        final_pred[i]=43\n        count+=1\ncount","metadata":{"execution":{"iopub.status.busy":"2021-12-03T22:19:34.617365Z","iopub.execute_input":"2021-12-03T22:19:34.618148Z","iopub.status.idle":"2021-12-03T22:19:34.656882Z","shell.execute_reply.started":"2021-12-03T22:19:34.618089Z","shell.execute_reply":"2021-12-03T22:19:34.656105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count=0\nfor i in range(len(final_pred)):\n    if (final_pred[i]!=31 and y_pred3[i]==1 and y_pred1[i]==1):\n        final_pred[i]=31\n        count+=1\n    elif (final_pred[i]!=41 and y_pred4[i]==1 and y_pred1[i]==1):\n        final_pred[i]=41\n        count+=1\n    elif (final_pred[i]!=32 and y_pred3[i]==1 and y_pred2[i]==1):\n        final_pred[i]=32\n        count+=1\n    elif (final_pred[i]!=43 and y_pred3[i]==1 and y_pred4[i]==1):\n        final_pred[i]=43\n        count+=1\n    elif (final_pred[i]!=42 and y_pred2[i]==1 and y_pred4[i]==1):\n        final_pred[i]=42\n        count+=1\ncount","metadata":{"execution":{"iopub.status.busy":"2021-12-03T22:17:39.531032Z","iopub.execute_input":"2021-12-03T22:17:39.531889Z","iopub.status.idle":"2021-12-03T22:17:39.602195Z","shell.execute_reply.started":"2021-12-03T22:17:39.53185Z","shell.execute_reply":"2021-12-03T22:17:39.601609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count=0\nfor i in range(len(final_pred)):\n    if (final_pred[i]!=43 and (y_pred3[i]==1 or predictions3[i]==1)  and (y_pred4[i]==1 or predictions4[i]==1)):\n        final_pred[i]=43\n        count+=1\n    elif (final_pred[i]!=43 and predictions[i][2]>0.6 and predictions[i][3]>0.6):\n        final_pred[i]=43\n        count+=1\n    elif(final_pred[i]==41 and (y_pred3[i]==1 or predictions3[i]==1)):\n        final_pred[i]=43\n        count+=1\n    elif(final_pred[i]==42 and (y_pred3[i]==1 or predictions3[i]==1)):\n        final_pred[i]=43\n        count+=1\n    elif(final_pred[i]==32 and (y_pred4[i]==1 or predictions4[i]==1) ):\n        final_pred[i]=43\n        count+=1\n    elif(final_pred[i]==31 and (y_pred4[i]==1 or predictions4[i]==1) ):\n        final_pred[i]=43\n        count+=1\n    #elif((y_pred3[i]!=1 or predictions3[i]!=1) and (y_pred4[i]!=1) and final_pred[i]!=21):\n     #   final_pred[i]=21\n      #  count+=1\n        \n#or predictions4[i]==1\ncount","metadata":{"execution":{"iopub.status.busy":"2021-12-06T03:32:11.018496Z","iopub.execute_input":"2021-12-06T03:32:11.018775Z","iopub.status.idle":"2021-12-06T03:32:11.100747Z","shell.execute_reply.started":"2021-12-06T03:32:11.018744Z","shell.execute_reply":"2021-12-06T03:32:11.099880Z"},"trusted":true},"execution_count":53,"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"1228"},"metadata":{}}]},{"cell_type":"code","source":"count=0\nfor i in range(len(final_pred)):\n    if (final_pred[i]!=43 and predictions3[i]==1 and predictions4[i]==1):\n        final_pred[i]=43\n        count+=1\ncount","metadata":{"execution":{"iopub.status.busy":"2021-12-04T00:02:56.783283Z","iopub.execute_input":"2021-12-04T00:02:56.784075Z","iopub.status.idle":"2021-12-04T00:02:56.7977Z","shell.execute_reply.started":"2021-12-04T00:02:56.78401Z","shell.execute_reply":"2021-12-04T00:02:56.796617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_submission = pd.DataFrame({'ID': aid, 'Label': final_pred})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submissions.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T03:33:14.872665Z","iopub.execute_input":"2021-12-06T03:33:14.873031Z","iopub.status.idle":"2021-12-06T03:33:14.948305Z","shell.execute_reply.started":"2021-12-06T03:33:14.872996Z","shell.execute_reply":"2021-12-06T03:33:14.947412Z"},"trusted":true},"execution_count":54,"outputs":[]}]}